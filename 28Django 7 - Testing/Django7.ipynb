{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Django 7 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are automated tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests are simple routines that check the operation of your code. Testing operates at different levels. Some tests might apply to a tiny detail (does a particular model method return values as expected?) while others examine the overall operation of the software (does a sequence of user inputs on the site produce the desired result?). That’s no different from the kind of testing you did earlier in Django 3 Tutorial, using the shell to examine the behavior of a method, or running the application and entering data to check how it behaves.\n",
    "\n",
    "What’s different in automated tests is that the testing work is done for you by the system. You create a set of tests once, and then as you make changes to your app, you can check that your code still works as you originally intended, without having to perform time consuming manual testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you need to create tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may feel that ‘checking that my web application seems to work’ will be a satisfactory test in a work environment. However, when working with sophisticated application ‘checking that my web application seems to work’ is not good enough. In a complex application you might have dozens of complex/obscure interactions between components. A change in any of those components could have unexpected/unintended consequences on the application’s behavior. Checking that it still ‘seems to work’ could mean running through your code’s functionality with twenty different variations of your test data just to make sure you haven’t broken something - not a good use of your time.\n",
    "\n",
    "That’s especially true when automated tests could do this for you in seconds. If something’s gone wrong, tests will also assist in identifying the code that’s causing the unexpected behavior. Sometimes it may seem a chore to do the unglamorous and unexciting business of writing tests, particularly when you know your code is working properly. However, the task of writing tests is more fulfilling than spending hours testing your application manually or trying to identify the cause of a newly-introduced problem. \n",
    "\n",
    "Additionally, when working in complex applications maintained by teams, tests guarantee that colleagues don’t inadvertently break your code (and that you don’t break theirs without knowing). If you want to make a living as a programmer, you must be good at writing tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic testing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to approach writing tests.\n",
    "\n",
    "Some programmers follow a discipline called “test-driven development”. This means that the programmer actually writes tests before he writes his code. More often, a newcomer to testing will create some code and later decide that it should have some tests. Perhaps it would have been better to write some tests earlier, but it’s never too late to get started. Sometimes you're working on code you know is reliable. In such a case, it’s fruitful to write your first test the next time you make a change, either when you add a new feature or fix a bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing a bug in or Blog application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that for whatever reason you want to programmatically check whether a post was published recently or not. We create a new method in the Post model (in *blog/models.py*) to check whether a certain post was published within the last 24 hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def was_published_recently(self):\n",
    "    return self.published_date >= timezone.now() - datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, there’s a bug in our blog application: the Post.was_published_recently() method returns True if the Question was published within the last day (which is correct) but also if the Post’s pub_date field is in the future (which certainly isn’t correct).\n",
    "\n",
    "To check if the bug really exists, using the Admin create a Post whose date lies in the future and check the method using the shell:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> python manage.py shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import datetime\n",
    ">>> from django.utils import timezone\n",
    ">>> from blog.models import Post\n",
    ">>> # create a Post instance with pub_date 30 days in the future\n",
    ">>> future_post = Post(published_date=timezone.now() + datetime.timedelta(days=30))\n",
    ">>> future_post.was_published_recently()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get a result of True from running the previous commands in the shell.  So you want to fix that bug, but you also want to have a test that ensures that this specific bug is not accidentally introduced in the future by some developer that modifies your data model or some other part of your blog application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test to expose the bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we’ve just done in the shell to test for the problem is exactly what we can do automate in a test, so let’s turn that code into an automated test.\n",
    "\n",
    "A conventional place for an application’s tests is in the application’s tests.py file; the testing system will automatically find tests in any file whose name begins with test.\n",
    "\n",
    "Place the following code in *blog/tests.py*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from django.utils import timezone\n",
    "from django.test import TestCase\n",
    "from .models import Post\n",
    "\n",
    "\n",
    "class PostMethodTests(TestCase):\n",
    "    def test_was_published_recently_with_future_post(self):\n",
    "        \"\"\"\n",
    "        method was_published_recently() in Post model should return False for questions whose\n",
    "        pub_date is in the future.\n",
    "        \"\"\"\n",
    "        time = timezone.now() + datetime.timedelta(days=30)\n",
    "        future_question = Post(pub_date=time)\n",
    "        self.assertEqual(future_question.was_published_recently(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done here is created a django.test.TestCase subclass with a method that creates a Post instance with a pub_date in the future. We then check the output of was_published_recently() - which ought to be False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal, we can run our test:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> python manage.py test blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you’ll see something like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating test database for alias 'default'...\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.010s\n",
    "\n",
    "FAILED (errors=1)\n",
    "Destroying test database for alias 'default'...\n",
    "\n",
    "(python35) c:\\workspace\\mysite>python manage.py test blog\n",
    "Creating test database for alias 'default'...\n",
    "F\n",
    "======================================================================\n",
    "FAIL: test_was_published_recently_with_future_post (blog.tests.PostMethodTests)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"c:\\workspace\\mysite\\blog\\tests.py\", line 14, in test_was_published_recen\n",
    "tly_with_future_post\n",
    "    self.assertEqual(future_question.was_published_recently(), False)\n",
    "AssertionError: True != False\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.000s\n",
    "\n",
    "FAILED (failures=1)\n",
    "Destroying test database for alias 'default'..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened was this:\n",
    "\n",
    "- python manage.py test blog looked for tests in the blog application\n",
    "- it found a subclass of the django.test.TestCase class\n",
    "- it created a special database for the purpose of testing\n",
    "- it looked for test methods - ones whose names begin with test\n",
    "- in test_was_published_recently_with_future_post it created a Post instance whose published_date is 30 days in the future\n",
    "- using the assertEqual() method, it discovered that its was_published_recently() returns True, though we wanted it to return False\n",
    "- The test informs us which test failed and even the line on which the failure occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amend the method was_published_recently of the Post model in *models.py*, so that it will only return True if the date is also in the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def was_published_recently(self):\n",
    "    now = timezone.now()\n",
    "    return now - datetime.timedelta(days=1) <= self.published_date <= now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can run our test again:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> python manage.py test blog"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating test database for alias 'default'...\n",
    ".\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.000s\n",
    "\n",
    "OK\n",
    "Destroying test database for alias 'default'..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time out automated test didn't raise any AssertionError. Congratulations! you successfully implemented your first test in Django. Let's recap what we have done:\n",
    "\n",
    "After identifying a bug, we wrote a test that exposes it and corrected the bug in the code so our test passes. Many other things might go wrong with our application in the future, but we can be sure that we won’t inadvertently reintroduce this bug, because simply running the test will warn us about it immediately. We can consider this little portion of the application pinned down safely forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying our progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s upload today’s practical advances to our production server. Once again, from your git console type:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> git status\n",
    "> git add -A .\n",
    "> git status\n",
    "> git commit -m \"Added a test to our blog application.\"\n",
    "> git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, log back in to https://www.pythonanywhere.com/ and go to your Bash console (or start a new one), and run:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> cd ~/my-first-blog\n",
    "> source myvenv/bin/activate \n",
    "> git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, hop on over to the Web tab and hit <font color=\"green\">**Reload** </font> on your web app. Your update should be live!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
